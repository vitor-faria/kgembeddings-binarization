
@inproceedings{beniwal_data_2018,
	title = {Data {Mining} with {Linked} {Data}: {Past}, {Present}, and {Future}},
	shorttitle = {Data {Mining} with {Linked} {Data}},
	doi = {10.1109/ICCMC.2018.8487861},
	abstract = {Linked Data has emerged as a popular method for representing structured data. One of the prime aims is to convert today's web of documents into a web of data where the data is machine-readable as well as processable. This research paper focuses on the data mining techniques used for mining the raw data. However, these techniques are cumbersome and can be optimized using Linked Data. Hence, we discuss the data mining techniques with Linked Data that may play a pivotal role in future in extracting meaningful information from unstructured or semi-structured data.},
	booktitle = {2018 {Second} {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	author = {Beniwal, Rohit and Gupta, Vikas and Rawat, Manish and Aggarwal, Rishabh},
	month = feb,
	year = {2018},
	keywords = {Conferences, Data mining, Data Mining, KDD, Knowledge discovery, Linked data, Linked Data, Resource description framework, Social Media Data Mining, Social network services, Web of Data},
	pages = {1031--1035},
	file = {IEEE Xplore Abstract Record:/Users/vitorfaria/Zotero/storage/RXEC4PUT/authors.html:text/html},
}

@article{ristoski_rdf2vec_2019,
	title = {{RDF2Vec}: {RDF} graph embeddings and their applications},
	volume = {10},
	issn = {22104968, 15700844},
	shorttitle = {{RDF2Vec}},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-180317},
	doi = {10.3233/SW-180317},
	abstract = {Linked Open Data has been recognized as a valuable source for background information in many data mining and information retrieval tasks. However, most of the existing tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. We evaluate our approach on three different tasks: (i) standard machine-learning tasks (ii) entity and document modeling (iii) content-based recommender systems. The evaluation shows that the proposed entity embeddings outperform existing techniques, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.},
	language = {en},
	number = {4},
	urldate = {2023-01-26},
	journal = {Semantic Web},
	author = {Ristoski, Petar and Rosati, Jessica and Di Noia, Tommaso and De Leone, Renato and Paulheim, Heiko},
	editor = {Lecue, Freddy},
	month = may,
	year = {2019},
	pages = {721--752},
	annote = {RDF2Vec embeddings are suitable for document similarity and outperforms some purely text-based similarity approaches
},
	file = {Ristoski et al. - 2019 - RDF2Vec RDF graph embeddings and their applicatio.pdf:/Users/vitorfaria/Zotero/storage/HMYJ6XJ7/Ristoski et al. - 2019 - RDF2Vec RDF graph embeddings and their applicatio.pdf:application/pdf},
}

@misc{portisch_dlcc_2022,
	title = {The {DLCC} {Node} {Classification} {Benchmark} for {Analyzing} {Knowledge} {Graph} {Embeddings}},
	url = {http://arxiv.org/abs/2207.06014},
	abstract = {Knowledge graph embedding is a representation learning technique that projects entities and relations in a knowledge graph to continuous vector spaces. Embeddings have gained a lot of uptake and have been heavily used in link prediction and other downstream prediction tasks. Most approaches are evaluated on a single task or a single group of tasks to determine their overall performance. The evaluation is then assessed in terms of how well the embedding approach performs on the task at hand. Still, it is hardly evaluated (and often not even deeply understood) what information the embedding approaches are actually learning to represent. To fill this gap, we present the DLCC (Description Logic Class Constructors) benchmark, a resource to analyze embedding approaches in terms of which kinds of classes they can represent. Two gold standards are presented, one based on the real-world knowledge graph DBpedia and one synthetic gold standard. In addition, an evaluation framework is provided that implements an experiment protocol so that researchers can directly use the gold standard. To demonstrate the use of DLCC, we compare multiple embedding approaches using the gold standards. We find that many DL constructors on DBpedia are actually learned by recognizing different correlated patterns than those defined in the gold standard and that specific DL constructors, such as cardinality constraints, are particularly hard to be learned for most embedding approaches.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Portisch, Jan and Paulheim, Heiko},
	month = jul,
	year = {2022},
	note = {arXiv:2207.06014 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at International Semantic Web Conference (ISWC) 2022},
	file = {arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/275YQR53/2207.html:text/html;Full Text PDF:/Users/vitorfaria/Zotero/storage/3HHF2AZF/Portisch e Paulheim - 2022 - The DLCC Node Classification Benchmark for Analyzi.pdf:application/pdf},
}

@article{portisch_knowledge_2022,
	title = {Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction – two sides of the same coin?},
	volume = {13},
	doi = {10.3233/SW-212892},
	abstract = {Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1) providing an encoding for data mining tasks, and (2) predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.},
	journal = {Semantic Web},
	author = {Portisch, Jan and Heist, Nicolas and Paulheim, Heiko},
	month = jan,
	year = {2022},
	pages = {1--24},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/V8WSIC8H/Portisch et al. - 2022 - Knowledge graph embedding for data mining vs. know.pdf:application/pdf},
}

@article{tissier_near-lossless_2019,
	title = {Near-lossless {Binarization} of {Word} {Embeddings}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/1803.09065},
	doi = {10.1609/aaai.v33i01.33017104},
	abstract = {Word embeddings are commonly used as a starting point in many NLP models to achieve state-of-the-art performances. However, with a large vocabulary and many dimensions, these floating-point representations are expensive both in terms of memory and calculations which makes them unsuitable for use on low-resource devices. The method proposed in this paper transforms real-valued embeddings into binary embeddings while preserving semantic information, requiring only 128 or 256 bits for each vector. This leads to a small memory footprint and fast vector operations. The model is based on an autoencoder architecture, which also allows to reconstruct original vectors from the binary ones. Experimental results on semantic similarity, text classification and sentiment analysis tasks show that the binarization of word embeddings only leads to a loss of {\textasciitilde}2\% in accuracy while vector size is reduced by 97\%. Furthermore, a top-k benchmark demonstrates that using these binary vectors is 30 times faster than using real-valued vectors.},
	number = {01},
	urldate = {2024-01-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tissier, Julien and Gravier, Christophe and Habrard, Amaury},
	month = jul,
	year = {2019},
	note = {arXiv:1803.09065 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {7104--7111},
	annote = {Comment: Accepted as a long paper at AAAI 2019},
	file = {arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/VHUKYFWW/1803.html:text/html;Full Text PDF:/Users/vitorfaria/Zotero/storage/EYBFGJ5B/Tissier et al. - 2019 - Near-lossless Binarization of Word Embeddings.pdf:application/pdf},
}

@incollection{pellegrino_geval_2020,
	title = {{GEval}: {A} {Modular} and {Extensible} {Evaluation} {Framework} for {Graph} {Embedding} {Techniques}},
	isbn = {978-3-030-49460-5},
	shorttitle = {{GEval}},
	abstract = {While RDF data are graph shaped by nature, most traditional Machine Learning (ML) algorithms expect data in a vector form. To transform graph elements to vectors, several graph embedding approaches have been proposed. Comparing these approaches is interesting for 1) developers of new embedding techniques to verify in which cases their proposal outperforms the state-of-art and 2) consumers of these techniques in choosing the best approach according to the task(s) the vectors will be used for. The comparison could be delayed (and made difficult) by the choice of tasks, the design of the evaluation, the selection of models, parameters, and needed datasets. We propose GEval, an evaluation framework to simplify the evaluation and the comparison of graph embedding techniques. The covered tasks range from ML tasks (Classification, Regression, Clustering), semantic tasks (entity relatedness, document similarity) to semantic analogies. However, GEval is designed to be (easily) extensible. In this article, we will describe the design and development of the proposed framework by detailing its overall structure, the already implemented tasks, and how to extend it. In conclusion, to demonstrate its operating approach, we consider the parameter tuning of the KGloVe algorithm as a use case.},
	author = {Pellegrino, Maria and Altabba, Abdulrahman and Garofalo, Martina and Ristoski, Petar and Cochez, Michael},
	month = may,
	year = {2020},
	doi = {10.1007/978-3-030-49461-2_33},
	pages = {565--582},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/28GKWB2B/Pellegrino et al. - 2020 - GEval A Modular and Extensible Evaluation Framewo.pdf:application/pdf},
}

@misc{portisch_kgvec2go_2020,
	title = {{KGvec2go} -- {Knowledge} {Graph} {Embeddings} as a {Service}},
	url = {http://arxiv.org/abs/2003.05809},
	doi = {10.48550/arXiv.2003.05809},
	abstract = {In this paper, we present KGvec2go, a Web API for accessing and consuming graph embeddings in a light-weight fashion in downstream applications. Currently, we serve pre-trained embeddings for four knowledge graphs. We introduce the service and its usage, and we show further that the trained models have semantic value by evaluating them on multiple semantic benchmarks. The evaluation also reveals that the combination of multiple models can lead to a better outcome than the best individual model.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Portisch, Jan and Hladik, Michael and Paulheim, Heiko},
	month = mar,
	year = {2020},
	note = {arXiv:2003.05809 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Databases},
	annote = {Comment: to be published in the Proceedings of the International Conference on Language Resources and Evaluation (LREC) 2020},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/VY3D6GY3/Portisch et al. - 2020 - KGvec2go -- Knowledge Graph Embeddings as a Servic.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/BATD2659/2003.html:text/html},
}

@misc{portisch_rdf2vec_2023,
	title = {The {RDF2vec} {Family} of {Knowledge} {Graph} {Embedding} {Methods} {\textbar} www.semantic-web-journal.net},
	url = {https://www.semantic-web-journal.net/content/rdf2vec-family-knowledge-graph-embedding-methods-1},
	abstract = {Knowledge graph embeddings represent a group of machine learning techniques which project entities and relations of a knowledge graph to continuous vector spaces. RDF2vec is a scalable embedding approach rooted in the combination of random walks with a language model. It has been successfully used in various applications. Recently, multiple variants to the RDF2vec approach have been proposed, introducing variations both on the walk generation and on the language modeling side.
The combination of those different approaches has lead to an increasing family of RDF2vec variants. In this paper, we evaluate a total of twelve RDF2vec variants on a comprehensive set of benchmark models, and compare them to seven existing knowledge graph embedding methods from the family of link prediction approaches. Besides the established
GEval benchmark introducing various downstream machine learning tasks on the DBpedia knowledge graph, we also use the new DLCC (Description Logic Class Constructors) benchmark consisting of two gold standards, one based on DBpedia, and one based on synthetically generated graphs. The latter allows for analyzing which ontological patterns in a knowledge graph can actually be learned by different embedding.
With this evaluation, we observe that certain tailored RDF2vec variants can lead to improved performance on different downstream tasks, given the nature of the underlying problem, and that they, in particular, have a different behavior in modeling similarity and relatedness. The findings can be used to provide guidance in selecting a particular RDF2vec method for a given task},
	urldate = {2024-01-09},
	author = {Portisch, Jan and Paulheim, Heiko},
	month = aug,
	year = {2023},
	file = {The RDF2vec Family of Knowledge Graph Embedding Methods | www.semantic-web-journal.net:/Users/vitorfaria/Zotero/storage/V3UN6FDN/rdf2vec-family-knowledge-graph-embedding-methods-1.html:text/html},
}

@misc{portisch_putting_2021,
	title = {Putting {RDF2vec} in {Order}},
	url = {http://arxiv.org/abs/2108.05280},
	doi = {10.48550/arXiv.2108.05280},
	abstract = {The RDF2vec method for creating node embeddings on knowledge graphs is based on word2vec, which, in turn, is agnostic towards the position of context words. In this paper, we argue that this might be a shortcoming when training RDF2vec, and show that using a word2vec variant which respects order yields considerable performance gains especially on tasks where entities of different classes are involved.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Portisch, Jan and Paulheim, Heiko},
	month = aug,
	year = {2021},
	note = {arXiv:2108.05280 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted at the ISWC 2021 posters and demos track},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/2GX37CVJ/Portisch e Paulheim - 2021 - Putting RDF2vec in Order.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/LRE96CPQ/2108.html:text/html},
}

@inproceedings{ling_twotoo_2015,
	title = {Two/{Too} {Simple} {Adaptations} of {Word2Vec} for {Syntax} {Problems}},
	doi = {10.3115/v1/N15-1142},
	abstract = {We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-of-speech tagging and dependency parsing using our proposed models.},
	author = {Ling, Wang and Dyer, Chris and Black, Alan and Trancoso, Isabel},
	month = may,
	year = {2015},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/VJB99VJ4/Ling et al. - 2015 - TwoToo Simple Adaptations of Word2Vec for Syntax .pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/YQUBDKCE/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/YGFTS7VC/1301.html:text/html},
}

@article{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {2013},
	abstract = {We consider the problem of embedding entities and relationships of multi relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	month = dec,
	year = {2013},
	annote = {TransE paper
},
}

@article{lin_learning_2015,
	title = {Learning {Entity} and {Relation} {Embeddings} for {Knowledge} {Graph} {Completion}},
	volume = {29},
	doi = {10.1609/aaai.v29i1.9491},
	abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.},
	journal = {Proceedings of AAAI},
	author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
	month = feb,
	year = {2015},
	pages = {2181--2187},
	annote = {TransR paper
},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/LLY7ZSY4/Lin et al. - 2015 - Learning Entity and Relation Embeddings for Knowle.pdf:application/pdf},
}

@misc{sun_rotate_2019,
	title = {{RotatE}: {Knowledge} {Graph} {Embedding} by {Relational} {Rotation} in {Complex} {Space}},
	shorttitle = {{RotatE}},
	url = {http://arxiv.org/abs/1902.10197},
	doi = {10.48550/arXiv.1902.10197},
	abstract = {We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
	month = feb,
	year = {2019},
	note = {arXiv:1902.10197 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to ICLR 2019},
	annote = {RotatE paper
},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/D6F5J5JC/Sun et al. - 2019 - RotatE Knowledge Graph Embedding by Relational Ro.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/2DUK75XA/1902.html:text/html},
}

@inproceedings{nickel_three-way_2011,
	title = {A {Three}-{Way} {Model} for {Collective} {Learning} on {Multi}-{Relational} {Data}.},
	abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
	author = {Nickel, Maximilian and Tresp, Volker and Kröger, Peer},
	month = jan,
	year = {2011},
	pages = {809--816},
	annote = {RESCAL paper
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/VUKR8GBZ/Nickel et al. - 2011 - A Three-Way Model for Collective Learning on Multi.pdf:application/pdf},
}

@misc{trouillon_complex_2016,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	url = {http://arxiv.org/abs/1606.06357},
	doi = {10.48550/arXiv.1606.06357},
	abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier, Éric and Bouchard, Guillaume},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06357 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10+2 pages, accepted at ICML 2016},
	annote = {ComplEx paper
},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/33Z2RBNS/Trouillon et al. - 2016 - Complex Embeddings for Simple Link Prediction.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/XW7VJT8W/1606.html:text/html},
}

@misc{yang_embedding_2014,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {https://arxiv.org/abs/1412.6575v4},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	language = {en},
	urldate = {2024-01-10},
	journal = {arXiv.org},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = dec,
	year = {2014},
	annote = {DistMult paper
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/249SFV5I/Yang et al. - 2014 - Embedding Entities and Relations for Learning and .pdf:application/pdf},
}

@misc{cai_comprehensive_2018,
	title = {A {Comprehensive} {Survey} of {Graph} {Embedding}: {Problems}, {Techniques} and {Applications}},
	shorttitle = {A {Comprehensive} {Survey} of {Graph} {Embedding}},
	url = {http://arxiv.org/abs/1709.07604},
	doi = {10.48550/arXiv.1709.07604},
	abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Cai, Hongyun and Zheng, Vincent W. and Chang, Kevin Chen-Chuan},
	month = feb,
	year = {2018},
	note = {arXiv:1709.07604 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: A 20-page comprehensive survey of graph/network embedding for over 150+ papers till year 2018. It provides systematic categorization of problems, techniques and applications. Accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE). Comments and suggestions are welcomed for continuously improving this survey},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/8TDHNNPW/Cai et al. - 2018 - A Comprehensive Survey of Graph Embedding Problem.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/G9IPJTJ2/1709.html:text/html},
}

@article{zou_survey_2020,
	title = {A {Survey} on {Application} of {Knowledge} {Graph}},
	volume = {1487},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/1487/1/012016},
	doi = {10.1088/1742-6596/1487/1/012016},
	abstract = {Knowledge graphs, representation of information as a semantic graph, have caused wide concern in both industrial and academic world. Their property of providing semantically structured information has brought important possible solutions for many tasks including question answering, recommendation and information retrieval, and is considered to offer great promise for building more intelligent machines by many researchers. Although knowledge graphs have already supported multiple “Big Data” applications in all sorts of commercial and scientific domains since Google coined this term in 2012, there was no previous study give a systemically review of the application of knowledge graphs. Therefore, unlike other related work which focuses on the construction techniques of knowledge graphs, this present paper aims at providing a first survey on these applications stemming from different domains. This paper also points out that while important advancements of applying knowledge graphs’ great ability of providing semantically structured information into specific domains have been made in recent years, several aspects still remain to be explored.},
	language = {en},
	number = {1},
	urldate = {2024-01-11},
	journal = {Journal of Physics: Conference Series},
	author = {Zou, Xiaohan},
	month = mar,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {012016},
	file = {IOP Full Text PDF:/Users/vitorfaria/Zotero/storage/J7FH443Q/Zou - 2020 - A Survey on Application of Knowledge Graph.pdf:application/pdf},
}

@article{berners-lee_semantic_2001,
	title = {The {Semantic} {Web}: {A} {New} {Form} of {Web} {Content} {That} is {Meaningful} to {Computers} {Will} {Unleash} a {Revolution} of {New} {Possibilities}},
	shorttitle = {The {Semantic} {Web}},
	journal = {ScientificAmerican.com},
	author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
	month = may,
	year = {2001},
	annote = {Semantic web
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/S8JU9UVA/Berners-Lee et al. - 2001 - The Semantic Web A New Form of Web Content That i.pdf:application/pdf},
}

@article{ferrucci_building_2010,
	title = {Building {Watson}: {An} {Overview} of the {DeepQA} {Project}},
	volume = {31},
	shorttitle = {Building {Watson}},
	doi = {10.1609/aimag.v31i3.2303},
	abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV quiz show, Jeopardy. The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After three years of intense research and development by a core team of about 20 researchers, Watson is performing at human expert levels in terms of precision, confidence, and speed at the Jeopardy quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that can be used as a foundation for combining, deploying, evaluating, and advancing a wide range of algorithmic techniques to rapidly advance the field of question answering (QA).},
	journal = {AI Magazine},
	author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Christopher},
	month = sep,
	year = {2010},
	pages = {59--79},
	annote = {IBM Watson
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/XGC8XEC2/Ferrucci et al. - 2010 - Building Watson An Overview of the DeepQA Project.pdf:application/pdf},
}

@inproceedings{liu_combining_2019,
	title = {Combining {Enterprise} {Knowledge} {Graph} and {News} {Sentiment} {Analysis} for {Stock} {Price} {Prediction}},
	url = {http://hdl.handle.net/10125/59565},
	doi = {10.24251/HICSS.2019.153},
	abstract = {Semantic Scholar extracted view of "Combining Enterprise Knowledge Graph and News Sentiment Analysis for Stock Price Prediction" by Jue Liu et al.},
	urldate = {2024-01-11},
	author = {Liu, Jue and Lu, Zhuocheng and Du, Wei},
	year = {2019},
	annote = {stock price predicition
},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/H3VLVS3U/Liu et al. - 2019 - Combining Enterprise Knowledge Graph and News Sent.pdf:application/pdf},
}

@article{lehmann_dbpedia_2015,
	title = {{DBpedia} – {A} large-scale, multilingual knowledge base extracted from {Wikipedia}},
	volume = {6},
	issn = {15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-140134},
	doi = {10.3233/SW-140134},
	abstract = {The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different language editions of Wikipedia. The largest DBpedia knowledge base which is extracted from the English edition of Wikipedia consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other 110 Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties. The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the different Wikipedia editions to be combined. The project publishes releases of all DBpedia knowledge bases for download and provides SPARQL query access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases, the project maintains a live knowledge base which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia data. Several hundred data sets on the Web publish RDF links pointing to DBpedia themselves and make DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications.},
	number = {2},
	urldate = {2024-01-11},
	journal = {Semantic Web},
	author = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N. and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, Sören and Bizer, Christian},
	year = {2015},
	pages = {167--195},
	annote = {[TLDR] An overview of the DBpedia community project is given, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications, including DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud.},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/HCI2M2X7/Lehmann et al. - 2015 - DBpedia – A large-scale, multilingual knowledge ba.pdf:application/pdf},
}

@inproceedings{suchanek_yago_2007,
	title = {Yago: a core of semantic knowledge},
	shorttitle = {Yago},
	abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95},
	author = {{Suchanek} and Fabian, Marián and {Kasneci} and {Gjergji} and {Weikum} and {Gerhard}},
	month = jan,
	year = {2007},
}

@article{vrandecic_wikidata_2014,
	title = {Wikidata: a free collaborative knowledgebase},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Wikidata},
	url = {https://dl.acm.org/doi/10.1145/2629489},
	doi = {10.1145/2629489},
	abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
	language = {en},
	number = {10},
	urldate = {2024-01-11},
	journal = {Communications of the ACM},
	author = {Vrandečić, Denny and Krötzsch, Markus},
	month = sep,
	year = {2014},
	pages = {78--85},
	annote = {[TLDR] This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else, to help improve the quality of the encyclopedia.},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/29QLUPBN/Vrandečić e Krötzsch - 2014 - Wikidata a free collaborative knowledgebase.pdf:application/pdf},
}

@misc{singhal_introducing_2012,
	title = {Introducing the {Knowledge} {Graph}: things, not strings},
	url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
	urldate = {2024-01-11},
	author = {Singhal, Amit},
	month = may,
	year = {2012},
	annote = {Google KG
},
	file = {Introducing the Knowledge Graph\: things, not strings | BibSonomy:/Users/vitorfaria/Zotero/storage/QZDB8VSV/fsteeg.html:text/html},
}

@misc{krishnan_making_2018,
	title = {Making search easier},
	url = {https://www.aboutamazon.com/news/innovation-at-amazon/making-search-easier},
	abstract = {How Amazon’s Product Graph is helping customers find products more easily.},
	language = {en},
	urldate = {2024-01-11},
	journal = {US About Amazon},
	author = {Krishnan, Arun},
	month = aug,
	year = {2018},
	note = {Section: Innovation at Amazon},
	annote = {Amazon KG
},
	file = {Snapshot:/Users/vitorfaria/Zotero/storage/WEMDHCMD/making-search-easier.html:text/html},
}

@misc{shrivastava_bring_2017,
	title = {Bring rich knowledge of people, places, things and local businesses to your apps},
	url = {https://blogs.bing.com/search-quality-insights/2017-07/bring-rich-knowledge-of-people-places-things-and-local-businesses-to-your-apps},
	abstract = {Today we are excited to announce the availability of Bing Entity Search API, a new Microsoft Cognitive Service in Free Preview. While in Preview, it will be available within the US.},
	urldate = {2024-01-11},
	author = {Shrivastava, Saurabh},
	month = jul,
	year = {2017},
	annote = {Microsoft Bing KG
},
	file = {Snapshot:/Users/vitorfaria/Zotero/storage/8PZIU9J7/bring-rich-knowledge-of-people-places-things-and-local-businesses-to-your-apps.html:text/html},
}

@article{noy_industry-scale_2019,
	title = {Industry-scale {Knowledge} {Graphs}: {Lessons} and {Challenges}: {Five} diverse technology companies show how it’s done},
	volume = {17},
	shorttitle = {Industry-scale {Knowledge} {Graphs}},
	doi = {10.1145/3329781.3332266},
	abstract = {This article looks at the knowledge graphs of five diverse tech companies, comparing the similarities and differences in their respective experiences of building and using the graphs, and discussing the challenges that all knowledge-driven enterprises face today. The collection of knowledge graphs discussed here covers the breadth of applications, from search, to product descriptions, to social networks.},
	journal = {Queue},
	author = {Noy, Natasha and Gao, Yuqing and Jain, Anshu and Narayanan, Anant and Patterson, Alan and Taylor, Jamie},
	month = apr,
	year = {2019},
	pages = {48--75},
	annote = {Facebook, eBay, Google, Microsoft, IBM
},
}

@inproceedings{kern-isberner_one_2017,
	address = {Cham},
	title = {One {Knowledge} {Graph} to {Rule} {Them} {All}? {Analyzing} the {Differences} {Between} {DBpedia}, {YAGO}, {Wikidata} \& co.},
	volume = {10505},
	isbn = {978-3-319-67189-5 978-3-319-67190-1},
	shorttitle = {One {Knowledge} {Graph} to {Rule} {Them} {All}?},
	url = {http://link.springer.com/10.1007/978-3-319-67190-1_33},
	doi = {10.1007/978-3-319-67190-1_33},
	abstract = {Public Knowledge Graphs (KGs) on the Web are considered a valuable asset for developing intelligent applications. They contain general knowledge which can be used, e.g., for improving data analytics tools, text processing pipelines, or recommender systems. While the large players, e.g., DBpedia, YAGO, or Wikidata, are often considered similar in nature and coverage, there are, in fact, quite a few differences. In this paper, we quantify those differences, and identify the overlapping and the complementary parts of public KGs. From those considerations, we can conclude that the KGs are hardly interchangeable, and that each of them has its strenghts and weaknesses when it comes to applications in different domains.},
	urldate = {2024-01-12},
	publisher = {Springer International Publishing},
	author = {Ringler, Daniel and Paulheim, Heiko},
	editor = {Kern-Isberner, Gabriele and Fürnkranz, Johannes and Thimm, Matthias},
	year = {2017},
	doi = {10.1007/978-3-319-67190-1_33},
	note = {Book Title: KI 2017: Advances in Artificial Intelligence
Series Title: Lecture Notes in Computer Science},
	pages = {366--372},
	annote = {[TLDR] It is concluded that the KGs are hardly interchangeable, and that each has its strenghts and weaknesses when it comes to applications in different domains.},
}

@article{hogan_knowledge_2022,
	title = {Knowledge {Graphs}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2003.02320},
	doi = {10.1145/3447772},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	number = {4},
	urldate = {2024-01-12},
	journal = {ACM Computing Surveys},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = may,
	year = {2022},
	note = {arXiv:2003.02320 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
	pages = {1--37},
	annote = {Comment: Revision from v5: Correcting errata from previous version for entailment/models, and some other minor typos},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/CBG44GKK/Hogan et al. - 2022 - Knowledge Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/9NHX4WRV/2003.html:text/html},
}

@inproceedings{carlson_toward_2010,
	title = {Toward an {Architecture} for {Never}-{Ending} {Language} {Learning}.},
	volume = {3},
	abstract = {We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74\% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent. Copyright © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam and Mitchell, Tom},
	month = jan,
	year = {2010},
	annote = {NELL
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/E8LI6GU7/Carlson et al. - 2010 - Toward an Architecture for Never-Ending Language L.pdf:application/pdf},
}

@article{lenat_cyc_1998,
	title = {{CYC}: {A} {Large}-{Scale} {Investment} in {Knowledge} {Infrastructure}},
	volume = {38},
	shorttitle = {{CYC}},
	doi = {10.1145/219717.219745},
	abstract = {Since a decade ago, a person-century of effort has gone into building CYC, a universal schema of roughly 105 general concepts spanning human reality. Most of the time has been spent codifying knowledge about the concept; approximately 106 common sense axioms have been handcrafted for and entered into CYC's knowledge base, millions more have been inferred and cached by CYC. This paper studies the fundamental assumptions of doing such a large-scale project, reviews the technical lessons learned by the developers, and surveys the range of applications that are enabled by the technology.},
	journal = {Communications of the ACM},
	author = {Lenat, Douglas},
	month = dec,
	year = {1998},
	annote = {openCyc
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/KZ298PFK/Lenat - 1998 - CYC A Large-Scale Investment in Knowledge Infrast.pdf:application/pdf},
}

@inproceedings{bollacker_freebase_2008,
	address = {Vancouver Canada},
	title = {Freebase: a collaboratively created graph database for structuring human knowledge},
	isbn = {978-1-60558-102-6},
	shorttitle = {Freebase},
	url = {https://dl.acm.org/doi/10.1145/1376616.1376746},
	doi = {10.1145/1376616.1376746},
	abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
	language = {en},
	urldate = {2024-01-12},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
	month = jun,
	year = {2008},
	pages = {1247--1250},
	annote = {Freebase
},
	annote = {[TLDR] MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
}

@article{nickel_review_2016,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/1503.00759},
	doi = {10.1109/JPROC.2015.2483592},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
	number = {1},
	urldate = {2024-01-12},
	journal = {Proceedings of the IEEE},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	month = jan,
	year = {2016},
	note = {arXiv:1503.00759 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {11--33},
	annote = {Comment: To appear in Proceedings of the IEEE},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/SIDMHW2D/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowle.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/X4MX3PF2/1503.html:text/html},
}

@inproceedings{west_knowledge_2014,
	title = {Knowledge base completion via search-based question answering},
	doi = {10.1145/2566486.2568032},
	abstract = {Over the past few years, massive amounts of world knowledge have been accumulated in publicly available knowledge bases, such as Freebase, NELL, and YAGO. Yet despite their seemingly huge size, these knowledge bases are greatly incomplete. For example, over 70\% of people included in Freebase have no known place of birth, and 99\% have no known ethnicity. In this paper, we propose a way to leverage existing Web-search-based question-answering technology to fill in the gaps in knowledge bases in a targeted way. In particular, for each entity attribute, we learn the best set of queries to ask, such that the answer snippets returned by the search engine are most likely to contain the correct value for that attribute. For example, if we want to find Frank Zappa's mother, we could ask the query `who is the mother of Frank Zappa'. However, this is likely to return `The Mothers of Invention', which was the name of his band. Our system learns that it should (in this case) add disambiguating terms, such as Zappa's place of birth, in order to make it more likely that the search results contain snippets mentioning his mother. Our system also learns how many different queries to ask for each attribute, since in some cases, asking too many can hurt accuracy (by introducing false positives). We discuss how to aggregate candidate answers across multiple queries, ultimately returning probabilistic predictions for possible values for each attribute. Finally, we evaluate our system and show that it is able to extract a large number of facts with high confidence.},
	author = {West, Robert and Gabrilovich, Evgeniy and Murphy, Kevin and Sun, Shaohua and Gupta, Rahul and Lin, Dekang},
	month = apr,
	year = {2014},
	pages = {515--526},
}

@misc{xu_understanding_2020,
	title = {Understanding graph embedding methods and their applications},
	url = {http://arxiv.org/abs/2012.08019},
	doi = {10.48550/arXiv.2012.08019},
	abstract = {Graph analytics can lead to better quantitative understanding and control of complex networks, but traditional methods suffer from high computational cost and excessive memory requirements associated with the high-dimensionality and heterogeneous characteristics of industrial size networks. Graph embedding techniques can be effective in converting high-dimensional sparse graphs into low-dimensional, dense and continuous vector spaces, preserving maximally the graph structure properties. Another type of emerging graph embedding employs Gaussian distribution-based graph embedding with important uncertainty estimation. The main goal of graph embedding methods is to pack every node's properties into a vector with a smaller dimension, hence, node similarity in the original complex irregular spaces can be easily quantified in the embedded vector spaces using standard metrics. The generated nonlinear and highly informative graph embeddings in the latent space can be conveniently used to address different downstream graph analytics tasks (e.g., node classification, link prediction, community detection, visualization, etc.). In this Review, we present some fundamental concepts in graph analytics and graph embedding methods, focusing in particular on random walk-based and neural network-based methods. We also discuss the emerging deep learning-based dynamic graph embedding methods. We highlight the distinct advantages of graph embedding methods in four diverse applications, and present implementation details and references to open-source software as well as available databases in the Appendix for the interested readers to start their exploration into graph analytics.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Xu, Mengjia},
	month = dec,
	year = {2020},
	note = {arXiv:2012.08019 [cs, math]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: 27 pages, 13 figures},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/UEXB4N4Q/Xu - 2020 - Understanding graph embedding methods and their ap.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/CQDKE869/2012.html:text/html},
}

@misc{grover_node2vec_2016,
	title = {node2vec: {Scalable} {Feature} {Learning} for {Networks}},
	shorttitle = {node2vec},
	url = {http://arxiv.org/abs/1607.00653},
	doi = {10.48550/arXiv.1607.00653},
	abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Grover, Aditya and Leskovec, Jure},
	month = jul,
	year = {2016},
	note = {arXiv:1607.00653 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/2VQPUBGL/Grover e Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/NM4PAA6M/1607.html:text/html},
}

@inproceedings{krompass_querying_2014,
	title = {Querying {Factorized} {Probabilistic} {Triple} {Databases}},
	doi = {10.13140/2.1.2414.5920},
	abstract = {An increasing amount of data is becoming available in the form of large triple stores, with the Semantic Web's linked open data cloud (LOD) as one of the most prominent examples. Data quality and completeness are key issues in many community-generated data stores, like LOD, which motivates probabilistic and statistical approaches to data representation, reasoning and querying. In this paper we address the issue from the perspective of probabilistic databases, which account for uncertainty in the data via a probability distribution over all database instances. We obtain a highly compressed representation using the re-cently developed RESCAL approach and demonstrate experimentally that efficient querying can be obtained by exploiting inherent features of RESCAL via sub-query approximations of deterministic views.},
	author = {Krompass, Denis and Nickel, Maximilian and Tresp, Volker},
	month = oct,
	year = {2014},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/ZNSDDER9/Krompass et al. - 2014 - Querying Factorized Probabilistic Triple Databases.pdf:application/pdf},
}

@article{soergel_wordnet_1998,
	title = {{WordNet}. {An} {Electronic} {Lexical} {Database}},
	abstract = {This is a landmark book. For anyone interested in language, in dictionaries and thesauri, or
natural language processing, the introduction, Chapters 1- 4, and Chapter 16 are must reading.
(Select other chapters according to your special interests; see the chapter-by-chapter review).
These chapters provide a thorough introduction to the preeminent electronic lexical database of
today in terms of accessibility and usage in a wide range of applications. But what does that have
to do with digital libraries? Natural language processing is essential for dealing efficiently with
the large quantities of text now available online: fact extraction and summarization, automated
indexing and text categorization, and machine translation. Another essential function is helping
the user with query formulation through synonym relationships between words and hierarchical
and other relationships between concepts. WordNet supports both of these functions and thus
deserves careful study by the digital library community.},
	author = {Soergel, Dagobert},
	month = oct,
	year = {1998},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/PQ5NJQ69/Soergel - 1998 - WordNet. An Electronic Lexical Database.pdf:application/pdf},
}

@inproceedings{toutanova_observed_2015,
	title = {Observed {Versus} {Latent} {Features} for {Knowledge} {Base} and {Text} {Inference}},
	doi = {10.18653/v1/W15-4007},
	abstract = {In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18. We also compare latent and observed feature models on a more challenging dataset derived from FB15K, and additionally coupled with textual mentions from a web-scale corpus. We show that the observed features model is most effective at capturing the information present for entity pairs with textual relations, and a combination of the two combines the strengths of both model types.},
	author = {Toutanova, Kristina and Chen, Danqi},
	month = jul,
	year = {2015},
	annote = {FB15k-237 dataset variant
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/QN8XK8D9/Toutanova e Chen - 2015 - Observed Versus Latent Features for Knowledge Base.pdf:application/pdf},
}

@misc{dettmers_convolutional_2018,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	url = {http://arxiv.org/abs/1707.01476},
	doi = {10.48550/arXiv.1707.01476},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models -- which potentially limits performance. In this work, we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree -- which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set -- however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets -- deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across most datasets.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = jul,
	year = {2018},
	note = {arXiv:1707.01476 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended AAAI2018 paper},
	annote = {WN18RR and YAGO3-10 datasets
},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/BHTA2DDE/Dettmers et al. - 2018 - Convolutional 2D Knowledge Graph Embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/F59N7DSH/1707.html:text/html},
}

@misc{shi_open-world_2017,
	title = {Open-{World} {Knowledge} {Graph} {Completion}},
	url = {http://arxiv.org/abs/1711.03438},
	doi = {10.48550/arXiv.1711.03438},
	abstract = {Knowledge Graphs (KGs) have been applied to many tasks including Web search, link prediction, recommendation, natural language processing, and entity linking. However, most KGs are far from complete and are growing at a rapid pace. To address these problems, Knowledge Graph Completion (KGC) has been proposed to improve KGs by filling in its missing connections. Unlike existing methods which hold a closed-world assumption, i.e., where KGs are fixed and new entities cannot be easily added, in the present work we relax this assumption and propose a new open-world KGC task. As a first attempt to solve this task we introduce an open-world KGC model called ConMask. This model learns embeddings of the entity's name and parts of its text-description to connect unseen entities to the KG. To mitigate the presence of noisy text descriptions, ConMask uses a relationship-dependent content masking to extract relevant snippets and then trains a fully convolutional neural network to fuse the extracted snippets with entities in the KG. Experiments on large data sets, both old and new, show that ConMask performs well in the open-world KGC task and even outperforms existing KGC models on the standard closed-world KGC task.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Shi, Baoxu and Weninger, Tim},
	month = nov,
	year = {2017},
	note = {arXiv:1711.03438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 8 pages, accepted to AAAI 2018},
	annote = {DBpedia500k dataset
},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/RNMJEWGS/Shi e Weninger - 2017 - Open-World Knowledge Graph Completion.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/KZASDMF2/1711.html:text/html},
}

@misc{alshargi_concept2vec_2020,
	title = {Concept2vec: {Metrics} for {Evaluating} {Quality} of {Embeddings} for {Ontological} {Concepts}},
	shorttitle = {Concept2vec},
	url = {http://arxiv.org/abs/1803.04488},
	doi = {10.48550/arXiv.1803.04488},
	abstract = {Although there is an emerging trend towards generating embeddings for primarily unstructured data and, recently, for structured data, no systematic suite for measuring the quality of embeddings has been proposed yet. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of the encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework, multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts. We positioned our sampled data and code at https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.},
	urldate = {2024-01-14},
	publisher = {arXiv},
	author = {Alshargi, Faisal and Shekarpour, Saeedeh and Soru, Tommaso and Sheth, Amit},
	month = may,
	year = {2020},
	note = {arXiv:1803.04488 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.4, I.2.6},
	annote = {Comment: Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019)},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/JIL9BJCE/Alshargi et al. - 2020 - Concept2vec Metrics for Evaluating Quality of Emb.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/V4W7UWV3/1803.html:text/html},
}

@inproceedings{ristoski_collection_2016,
	title = {A {Collection} of {Benchmark} {Datasets} for {Systematic} {Evaluations} of {Machine} {Learning} on the {Semantic} {Web}},
	isbn = {978-3-319-46546-3},
	doi = {10.1007/978-3-319-46547-0_20},
	abstract = {In the recent years, several approaches for machine learning on the Semantic Web have been proposed. However, no extensive comparisons between those approaches have been undertaken, in particular due to a lack of publicly available, acknowledged benchmark datasets. In this paper, we present a collection of 22 benchmark datasets of different sizes. Such a collection of datasets can be used to conduct quantitative performance testing and systematic comparisons of approaches.},
	author = {Ristoski, Petar and Vries, Gerben Klaas Dirk and Paulheim, Heiko},
	month = oct,
	year = {2016},
	pages = {186--194},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/ECE9TAT7/Ristoski et al. - 2016 - A Collection of Benchmark Datasets for Systematic .pdf:application/pdf},
}

@inproceedings{melo_synthesizing_2017,
	title = {Synthesizing {Knowledge} {Graphs} for {Link} and {Type} {Prediction} {Benchmarking}},
	isbn = {978-3-319-58067-8},
	doi = {10.1007/978-3-319-58068-5_9},
	abstract = {Despite the growing amount of research in link and type prediction in knowledge graphs, systematic benchmark datasets are still scarce. In this paper, we propose a synthesis model for the generation of benchmark datasets for those tasks. Synthesizing data is a way of having control over important characteristics of the data, and allows the study of the impact of such characteristics on the performance of different methods. The proposed model uses existing knowledge graphs to create synthetic graphs with similar characteristics, such as distributions of classes, relations, and instances. As a first step, we replicate already existing knowledge graphs in order to validate the synthesis model. To do so, we perform extensive experiments with different link and type prediction methods. We show that we can systematically create knowledge graph benchmarks which allow for quantitative measurements of the result quality and scalability of link and type prediction methods.},
	author = {Meló, André and Paulheim, Heiko},
	month = may,
	year = {2017},
	pages = {136--151},
	file = {Versão submetida:/Users/vitorfaria/Zotero/storage/YMCT9LQU/Meló e Paulheim - 2017 - Synthesizing Knowledge Graphs for Link and Type Pr.pdf:application/pdf},
}

@incollection{bloem_kgbench_2021,
	title = {kgbench: {A} {Collection} of {Knowledge} {Graph} {Datasets} for {Evaluating} {Relational} and {Multimodal} {Machine} {Learning}},
	isbn = {978-3-030-77384-7},
	shorttitle = {kgbench},
	abstract = {Graph neural networks and other machine learning models offer a promising direction for machine learning on relational and multimodal data. Until now, however, progress in this area is difficult to gauge. This is primarily due to a limited number of datasets with (a) a high enough number of labeled nodes in the test set for precise measurement of performance, and (b) a rich enough variety of multimodal information to learn from. We introduce a set of new benchmark tasks for node classification on RDF-encoded knowledge graphs. We focus primarily on node classification, since this setting cannot be solved purely by node embedding models. For each dataset, we provide test and validation sets of at least 1000 instances, with some over 10000. Each task can be performed in a purely relational manner, or with multimodal information. All datasets are packaged in a CSV format that is easily consumable in any machine learning environment, together with the original source data in RDF and pre-processing code for full provenance. We provide code for loading the data into numpy and pytorch. We compute performance for several baseline models.},
	author = {Bloem, Peter and Wilcke, Xander and Berkel, Lucas and Boer, Victor},
	month = may,
	year = {2021},
	doi = {10.1007/978-3-030-77385-4_37},
	pages = {614--630},
	file = {Texto completo:/Users/vitorfaria/Zotero/storage/EFLFSSWD/Bloem et al. - 2021 - kgbench A Collection of Knowledge Graph Datasets .pdf:application/pdf},
}

@inproceedings{mahdisoltani_yago3_2015,
	title = {{YAGO3}: {A} {Knowledge} {Base} from {Multilingual} {Wikipedias}},
	shorttitle = {{YAGO3}},
	url = {https://www.semanticscholar.org/paper/YAGO3%3A-A-Knowledge-Base-from-Multilingual-Mahdisoltani-Biega/6c5b5adc3830ac45bf1d764603b1b71e5f729616},
	abstract = {We present YAGO3, an extension of the YAGO knowledge base that combines the information from the Wikipedias in multiple languages. Our technique fuses the multilingual information with the English WordNet to build one coherent knowledge base. We make use of the categories, the infoboxes, and Wikidata, and learn the meaning of infobox attributes across languages. We run our method on 10 different languages, and achieve a precision of 95\%-100\% in the attribute mapping. Our technique enlarges YAGO by 1m new entities and 7m new facts.},
	urldate = {2024-01-15},
	author = {Mahdisoltani, F. and Biega, J. and Suchanek, Fabian M.},
	year = {2015},
	annote = {[TLDR] This work fuses the multilingual information with the English WordNet to build one coherent knowledge base that combines the information from the Wikipedias in multiple languages, and enlarges YAGO by 1m new entities and 7m new facts.},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/U6D4CPI9/Mahdisoltani et al. - 2015 - YAGO3 A Knowledge Base from Multilingual Wikipedi.pdf:application/pdf},
}

@incollection{pellegrino_configurable_2019,
	title = {A {Configurable} {Evaluation} {Framework} for {Node} {Embedding} {Techniques}},
	isbn = {978-3-030-32326-4},
	abstract = {While Knowledge Graphs (KG) are graph shaped by nature, most traditional data mining and machine learning (ML) software expect data in a vector form. Several node embedding techniques have been proposed to represent each node in the KG as a low-dimensional feature vector. A node embedding technique should preferably be task independent. Therefore, when a new method has been developed, it should be tested on the tasks it was designed for as well as on other tasks. We present the design and implementation of a ready to use evaluation framework to simplify the node embedding technique testing phase. The provided tests range from ML tasks, semantic tasks to semantic analogies.},
	author = {Pellegrino, Maria and Cochez, Michael and Garofalo, Martina and Ristoski, Petar},
	month = oct,
	year = {2019},
	doi = {10.1007/978-3-030-32327-1_31},
	pages = {156--160},
}

@inproceedings{navali_word_2020,
	address = {Barcelona, Spain (Online)},
	title = {Word {Embedding} {Binarization} with {Semantic} {Information} {Preservation}},
	url = {https://aclanthology.org/2020.coling-main.108},
	doi = {10.18653/v1/2020.coling-main.108},
	abstract = {With growing applications of Machine Learning in daily lives Natural Language Processing (NLP) has emerged as a heavily researched area. Finding its applications in tasks ranging from simple Q/A chatbots to Fully fledged conversational AI, NLP models are vital. Word and Sentence embedding are one of the most common starting points of any NLP task. A word embedding represents a given word in a predefined vector-space while maintaining vector relations with similar or dis-similar entities. As such different pretrained embedding such as Word2Vec, GloVe, fasttext have been developed. These embedding generated on millions of words are however very large in terms of size. Having embedding with floating point precision also makes the downstream evaluation slow. In this paper we present a novel method to convert continuous embedding to its binary representation, thus reducing the overall size of the embedding while keeping the semantic and relational knowledge intact. This will facilitate an option of porting such big embedding onto devices where space is limited. We also present different approaches suitable for different downstream tasks based on the requirement of contextual and semantic information. Experiments have shown comparable result in downstream tasks with 7 to 15 times reduction in file size and about 5 \% change in evaluation parameters.},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Navali, Samarth and Sherki, Praneet and Inturi, Ramesh and Vala, Vanraj},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {1256--1265},
	annote = {cites Tissier
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/W6ZFVSNK/Navali et al. - 2020 - Word Embedding Binarization with Semantic Informat.pdf:application/pdf},
}

@article{van_der_maaten_visualizing_2008,
	title = {Visualizing data using t-{SNE}},
	volume = {9},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	journal = {Journal of Machine Learning Research},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	month = nov,
	year = {2008},
	pages = {2579--2605},
	annote = {t-SNE
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/KW2H8LMG/van der Maaten e Hinton - 2008 - Viualizing data using t-SNE.pdf:application/pdf},
}

@misc{draganov_unexplainable_2023,
	title = {Unexplainable {Explanations}: {Towards} {Interpreting} {tSNE} and {UMAP} {Embeddings}},
	shorttitle = {Unexplainable {Explanations}},
	url = {http://arxiv.org/abs/2306.11898},
	abstract = {It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Draganov, Andrew and Dohn, Simon},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11898 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/T2LPTPEK/2306.html:text/html;Full Text PDF:/Users/vitorfaria/Zotero/storage/RQDA2NLA/Draganov e Dohn - 2023 - Unexplainable Explanations Towards Interpreting t.pdf:application/pdf},
}

@inproceedings{charikar_similarity_2002,
	address = {Montreal Quebec Canada},
	title = {Similarity estimation techniques from rounding algorithms},
	isbn = {978-1-58113-495-7},
	url = {https://dl.acm.org/doi/10.1145/509907.509965},
	doi = {10.1145/509907.509965},
	abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family \${\textbackslash}F\$ of hash functions operating on a collection of objects, such that for two objects \textit{x,y}, \textbf{Pr}$_{\textrm{\textit{h}}}$εF[\textit{h}(\textit{x}) = \textit{h}(\textit{y})] = sim(\textit{x,y}), where \textit{sim}(\textit{x,y}) ε [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure \textit{sim}(\textit{A,B}) = {\textbackslash}frac\{{\textbar}A \&Pgr; B{\textbar}\}\{{\textbar}A \&Pgr B{\textbar}\}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:{\textless}ol{\textgreater}{\textless}li{\textgreater}A collection of vectors with the distance between → {\textbackslash}over \textit{u} and → {\textbackslash}over \textit{v} measured by Ø(→ {\textbackslash}over \textit{u}, → {\textbackslash}over \textit{v})/π, where Ø(→ {\textbackslash}over \textit{u}, → {\textbackslash}over \textit{v}) is the angle between → {\textbackslash}over \textit{u}) and → {\textbackslash}over \textit{v}). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.{\textless}/li{\textgreater}{\textless}li{\textgreater}A collection of distributions on \textit{n} points in a metric space, with distance between distributions measured by the Earth Mover Distance (\textbf{EMD}), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions \textit{P} and \textit{Q}, \textbf{EMD}(\textit{P,Q}) \&xie; \textbf{E}$_{\textrm{hε{\textbackslash}F}}$ [\textit{d}(\textit{h}(\textit{P}),\textit{h}(\textit{Q}))] \&xie; \textit{O}(log \textit{n} log log \textit{n}). \textbf{EMD}(\textit{P, Q}).{\textless}/li{\textgreater}{\textless}/ol{\textgreater}.},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the thiry-fourth annual {ACM} symposium on {Theory} of computing},
	publisher = {ACM},
	author = {Charikar, Moses S.},
	month = may,
	year = {2002},
	pages = {380--388},
	annote = {LSH - Locality sensitive hashing
},
	annote = {[TLDR] It is shown that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects.},
}

@misc{raunak_simple_2017,
	title = {Simple and {Effective} {Dimensionality} {Reduction} for {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1708.03629},
	doi = {10.48550/arXiv.1708.03629},
	abstract = {Word embeddings have become the basic building blocks for several natural language processing and information retrieval tasks. Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on further improving the pre-trained word vectors through post-processing algorithms. One such area of improvement is the dimensionality reduction of the word embeddings. Reducing the size of word embeddings through dimensionality reduction can improve their utility in memory constrained devices, benefiting several real-world applications. In this work, we present a novel algorithm that effectively combines PCA based dimensionality reduction with a recently proposed post-processing algorithm, to construct word embeddings of lower dimensions. Empirical evaluations on 12 standard word similarity benchmarks show that our algorithm reduces the embedding dimensionality by 50\%, while achieving similar or (more often) better performance than the higher dimension embeddings.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Raunak, Vikas},
	month = nov,
	year = {2017},
	note = {arXiv:1708.03629 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NIPS 2017 LLD Workshop},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/7I8DYYP9/Raunak - 2017 - Simple and Effective Dimensionality Reduction for .pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/KZDWP5MV/1708.html:text/html},
}

@inproceedings{mostard_semantic_2022,
	address = {New York, NY, USA},
	series = {{NLPIR} '21},
	title = {Semantic {Preserving} {Siamese} {Autoencoder} for {Binary} {Quantization} of {Word} {Embeddings}},
	isbn = {978-1-4503-8735-4},
	url = {https://dl.acm.org/doi/10.1145/3508230.3508235},
	doi = {10.1145/3508230.3508235},
	abstract = {Word embeddings are used as building blocks for a wide range of natural language processing and information retrieval tasks. These embeddings are usually represented as continuous vectors, requiring significant memory capacity and computationally expensive similarity measures. In this study, we introduce a novel method for semantic hashing continuous vector representations into lower-dimensional Hamming space while explicitly preserving semantic information between words. This is achieved by introducing a Siamese autoencoder combined with a novel semantic preserving loss function. We show that our quantization model induces only a 4\% loss of semantic information over continuous representations and outperforms the baseline models on several word similarity and sentence classification tasks. Finally, we show through cluster analysis that our method learns binary representations where individual bits hold interpretable semantic information. In conclusion, binary quantization of word embeddings significantly decreases time and space requirements while offering new possibilities through exploiting semantic information of individual bits in downstream information retrieval tasks.},
	urldate = {2024-01-17},
	booktitle = {Proceedings of the 2021 5th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Mostard, Wouter and Schomaker, Lambert and Wiering, Marco},
	month = mar,
	year = {2022},
	keywords = {Representation learning, Semantic hashing, Siamese autoencoder},
	pages = {30--38},
	annote = {Heiko’s suggestion. 4\% performance loss with binary word embeddings
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/S2DQ272X/Mostard et al. - 2022 - Semantic Preserving Siamese Autoencoder for Binary.pdf:application/pdf},
}

@misc{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv:1802.03426 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Reference implementation available at http://github.com/lmcinnes/umap},
	file = {arXiv Fulltext PDF:/Users/vitorfaria/Zotero/storage/XIQ3TCYN/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf:application/pdf;arXiv.org Snapshot:/Users/vitorfaria/Zotero/storage/A5NJ8BMR/1802.html:text/html},
}

@article{pan_relation_2021,
	title = {Relation {Reconstructive} {Binarization} of word embeddings},
	volume = {16},
	issn = {2095-2236},
	url = {https://doi.org/10.1007/s11704-021-0108-3},
	doi = {10.1007/s11704-021-0108-3},
	abstract = {Word-embedding acts as one of the backbones of modern natural language processing (NLP). Recently, with the need for deploying NLP models to low-resource devices, there has been a surge of interest to compress word embeddings into hash codes or binary vectors so as to save the storage and memory consumption. Typically, existing work learns to encode an embedding into a compressed representation from which the original embedding can be reconstructed. Although these methods aim to preserve most information of every individual word, they often fail to retain the relation between words, thus can yield large loss on certain tasks. To this end, this paper presents Relation Reconstructive Binarization (R2B) to transform word embeddings into binary codes that can preserve the relation between words. At its heart, R2B trains an auto-encoder to generate binary codes that allow reconstructing the word-by-word relations in the original embedding space. Experiments showed that our method achieved significant improvements over previous methods on a number of tasks along with a space-saving of up to 98.4\%. Specifically, our method reached even better results on word similarity evaluation than the uncompressed pre-trained embeddings, and was significantly better than previous compression methods that do not consider word relations.},
	language = {en},
	number = {2},
	urldate = {2024-01-21},
	journal = {Frontiers of Computer Science},
	author = {Pan, Feiyang and Li, Shuokai and Ao, Xiang and He, Qing},
	month = sep,
	year = {2021},
	keywords = {binary word embedding, embedding compression, variational auto-encoder},
	pages = {162307},
	annote = {cites Tissier
},
	file = {Full Text PDF:/Users/vitorfaria/Zotero/storage/WEH4N8EI/Pan et al. - 2021 - Relation Reconstructive Binarization of word embed.pdf:application/pdf},
}

@inproceedings{xu_convolutional_2015,
	address = {Buenos Aires, Argentina},
	series = {{IJCAI}'15},
	title = {Convolutional neural networks for text hashing},
	isbn = {978-1-57735-738-4},
	abstract = {Hashing, as a popular approximate nearest neighbor search, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning methods are utilized to learn similarity-preserving binary codes. However, most of them directly encode the explicit features, keywords, which fail to preserve the accurate semantic similarities in binary code beyond keyword matching, especially on short texts. Here we propose a novel text hashing framework with convolutional neural networks. In particular, we first embed the keyword features into compact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to learn the implicit features which are further incorporated with the explicit features to fit the pretrained binary code. Such base method can be successfully accomplished without any external tags/labels, and other three model variations are designed to integrate tags/labels. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods when tested on one short text dataset as well as one normal text dataset.},
	urldate = {2024-01-21},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Xu, Jiaming and Wang, Peng and Tian, Guanhua and Xu, Bo and Zhao, Jun and Wang, Fangyuan and Hao, Hongwei},
	month = jul,
	year = {2015},
	pages = {1369--1375},
	annote = {LSH semantic information
},
	file = {Xu et al. - Convolutional Neural Networks for Text Hashing.pdf:/Users/vitorfaria/Zotero/storage/YQLS766E/Xu et al. - Convolutional Neural Networks for Text Hashing.pdf:application/pdf},
}
